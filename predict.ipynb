{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52418c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äang táº£i mÃ´ hÃ¬nh...\n",
      "Táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng!\n",
      "\n",
      "1. Váº½ skeleton Ä‘Ã¨ lÃªn video 'output_hello_accurate.mp4'...\n",
      "2. Tiá»n xá»­ lÃ½ video Ä‘Ã£ cÃ³ skeleton...\n",
      "3. KÃ­ch thÆ°á»›c input cuá»‘i cÃ¹ng: torch.Size([1, 128, 3, 128, 128])\n",
      "4. Báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n...\n",
      "\n",
      "ðŸš€ Káº¾T QUáº¢ Dá»° ÄOÃN:\n",
      "==> Chá»‰ sá»‘ lá»›p dá»± Ä‘oÃ¡n: 35\n",
      "==> TÃªn lá»›p dá»± Ä‘oÃ¡n: thank you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# ===================================================================\n",
    "# BÆ¯á»šC 1: Äá»ŠNH NGHÄ¨A Láº I CÃC HÃ€M VÃ€ CLASS GIá»NG Há»†T KHI HUáº¤N LUYá»†N\n",
    "# ===================================================================\n",
    "\n",
    "# --- Äá»‹nh nghÄ©a kiáº¿n trÃºc mÃ´ hÃ¬nh ---\n",
    "MAX_FRAMES = 128\n",
    "RESIZE_TO = (128, 128)\n",
    "\n",
    "class VideoClassifier3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VideoClassifier3DCNN, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm3d(16), nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm3d(32), nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm3d(64), nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        fc_input_dim = 64 * (MAX_FRAMES // 8) * (RESIZE_TO[0] // 8) * (RESIZE_TO[1] // 8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fc_input_dim, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# --- HÃ m Má»šI: Váº½ Skeleton Ä‘Ã¨ lÃªn Video gá»‘c ---\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def create_overlay_video_from_path(video_path):\n",
    "    \"\"\"\n",
    "    Äá»c video, trÃ­ch xuáº¥t skeleton vÃ  váº½ nÃ³ TRá»°C TIáº¾P LÃŠN CÃC FRAME Gá»C.\n",
    "    Tráº£ vá» má»™t tensor PyTorch cÃ³ shape (num_frames, height, width, 3).\n",
    "    \"\"\"\n",
    "    holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    video_frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Táº¡o má»™t báº£n sao cá»§a frame Ä‘á»ƒ váº½ lÃªn, trÃ¡nh sá»­a Ä‘á»•i frame gá»‘c\n",
    "        frame_to_draw = frame.copy()\n",
    "        \n",
    "        # Chuyá»ƒn mÃ u vÃ  xá»­ lÃ½ báº±ng MediaPipe\n",
    "        image_rgb = cv2.cvtColor(frame_to_draw, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(image_rgb)\n",
    "        \n",
    "        # Váº½ skeleton Ä‘Ã¨ lÃªn frame\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame_to_draw, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1, circle_radius=1),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,0), thickness=1))\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame_to_draw, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1, circle_radius=1),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=1))\n",
    "        mp_drawing.draw_landmarks(\n",
    "            frame_to_draw, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=1, circle_radius=1),\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=1))\n",
    "        \n",
    "        # Chuyá»ƒn frame BGR (cá»§a OpenCV) sang RGB Ä‘á»ƒ lÆ°u vÃ o tensor\n",
    "        frame_rgb_output = cv2.cvtColor(frame_to_draw, cv2.COLOR_BGR2RGB)\n",
    "        video_frames.append(frame_rgb_output)\n",
    "\n",
    "    cap.release()\n",
    "    holistic.close()\n",
    "    \n",
    "    if not video_frames:\n",
    "        return torch.empty(0)\n",
    "        \n",
    "    return torch.from_numpy(np.array(video_frames))\n",
    "\n",
    "\n",
    "# --- HÃ m tiá»n xá»­ lÃ½ cuá»‘i cÃ¹ng (padding/normalize) ---\n",
    "def preprocess_single_video(video_tensor):\n",
    "    num_frames = video_tensor.shape[0]\n",
    "    if num_frames > MAX_FRAMES: video_tensor = video_tensor[:MAX_FRAMES]\n",
    "    elif num_frames < MAX_FRAMES:\n",
    "        padding = torch.zeros((MAX_FRAMES - num_frames,) + video_tensor.shape[1:], dtype=video_tensor.dtype)\n",
    "        video_tensor = torch.cat([video_tensor, padding], dim=0)\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize(RESIZE_TO),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    processed_frames = torch.stack([transform(frame.numpy()) for frame in video_tensor])\n",
    "    return processed_frames\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# BÆ¯á»šC 2: THá»°C HIá»†N Dá»° ÄOÃN\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_PATH = \"sign_language_3dcnn_final.pth\" \n",
    "    VIDEO_PATH = \"output_hello_accurate.mp4\"\n",
    "\n",
    "    CLASS_NAMES = [ 'again', 'bad', 'bathroom', 'book', 'busy', 'do not want', 'eat', 'father', 'fine', 'finish', 'forget', 'go', 'good', 'happy', 'hello', 'help', 'how', 'i', 'learn', 'like', 'meet', 'milk', 'more', 'mother', 'my', 'name', 'need', 'nice', 'no', 'please', 'question', 'right', 'sad', 'same', 'see you letter', 'thank you', 'want', 'what', 'when', 'where', 'which', 'who', 'why', 'wrong', 'yes', 'you', 'your' ]\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "    # --- Táº£i mÃ´ hÃ¬nh ---\n",
    "    print(\"Äang táº£i mÃ´ hÃ¬nh...\")\n",
    "    model = VideoClassifier3DCNN(num_classes=NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"Táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng!\")\n",
    "\n",
    "    # --- QUY TRÃŒNH Dá»° ÄOÃN ÄÃšNG ---\n",
    "    print(f\"\\n1. Váº½ skeleton Ä‘Ã¨ lÃªn video '{VIDEO_PATH}'...\")\n",
    "    overlay_video_tensor = create_overlay_video_from_path(VIDEO_PATH)\n",
    "\n",
    "    if overlay_video_tensor.shape[0] == 0:\n",
    "        print(\"Lá»—i: KhÃ´ng thá»ƒ xá»­ lÃ½ video Ä‘áº§u vÃ o.\")\n",
    "    else:\n",
    "        print(\"2. Tiá»n xá»­ lÃ½ video Ä‘Ã£ cÃ³ skeleton...\")\n",
    "        processed_tensor = preprocess_single_video(overlay_video_tensor)\n",
    "        \n",
    "\n",
    "        input_tensor = processed_tensor.unsqueeze(0).to(DEVICE)\n",
    "        print(f\"3. KÃ­ch thÆ°á»›c input cuá»‘i cÃ¹ng: {input_tensor.shape}\")\n",
    "        \n",
    "        print(\"4. Báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n...\")\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            _, predicted_idx = torch.max(output, 1)\n",
    "            predicted_class_index = predicted_idx.cpu().item()\n",
    "            predicted_class_name = CLASS_NAMES[predicted_class_index]\n",
    "\n",
    "        print(\"\\nðŸš€ Káº¾T QUáº¢ Dá»° ÄOÃN:\")\n",
    "        print(f\"==> Chá»‰ sá»‘ lá»›p dá»± Ä‘oÃ¡n: {predicted_class_index}\")\n",
    "        print(f\"==> TÃªn lá»›p dá»± Ä‘oÃ¡n: {predicted_class_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
