{
 "cells": [
  {
   "cell_type": "code",
   "id": "004010ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T16:58:59.958188Z",
     "start_time": "2025-11-10T16:58:57.137147Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# ===================================================================\n",
    "# B∆Ø·ªöC 1: ƒê·ªäNH NGHƒ®A L·∫†I KI·∫æN TR√öC V√Ä H√ÄM X·ª¨ L√ù\n",
    "# (PH·∫¢I GI·ªêNG H·ªÜT FILE HU·∫§N LUY·ªÜN)\n",
    "# ===================================================================\n",
    "\n",
    "# --- C√°c h·∫±ng s·ªë ph·∫£i kh·ªõp v·ªõi l√∫c hu·∫•n luy·ªán ---\n",
    "MAX_FRAMES = 128\n",
    "RESIZE_TO = (128, 128)\n",
    "\n",
    "# --- Ki·∫øn tr√∫c VideoTransformerClassifier (Copy t·ª´ file hu·∫•n luy·ªán) ---\n",
    "class VideoTransformerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=512, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(VideoTransformerClassifier, self).__init__()\n",
    "        \n",
    "        pretrained_cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        modules = list(pretrained_cnn.children())[:-1]\n",
    "        self.cnn_extractor = nn.Sequential(*modules)\n",
    "        \n",
    "        for param in self.cnn_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        cnn_output_dim = 512\n",
    "        self.projection = nn.Linear(cnn_output_dim, embed_dim)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, MAX_FRAMES + 1, embed_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        cnn_in = x.view(batch_size * num_frames, C, H, W)\n",
    "        cnn_out = self.cnn_extractor(cnn_in)\n",
    "        cnn_out = torch.flatten(cnn_out, 1)\n",
    "        frame_features = cnn_out.view(batch_size, num_frames, -1)\n",
    "        seq_in = self.projection(frame_features)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        seq_in = torch.cat((cls_tokens, seq_in), dim=1)\n",
    "        seq_in += self.positional_embedding\n",
    "        \n",
    "        transformer_out = self.transformer_encoder(seq_in)\n",
    "        cls_output = transformer_out[:, 0, :]\n",
    "        out = self.classifier(cls_output)\n",
    "        return out\n",
    "\n",
    "# --- H√†m ƒë·ªçc video t·ª´ file (d√πng OpenCV) ---\n",
    "def load_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    if not cap.isOpened():\n",
    "        print(f\"L·ªói: Kh√¥ng th·ªÉ m·ªü file video t·∫°i: {video_path}\")\n",
    "        return torch.empty(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Chuy·ªÉn t·ª´ BGR (OpenCV) sang RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "    cap.release()\n",
    "    # Tr·∫£ v·ªÅ tensor ·ªü d·∫°ng (T, H, W, C)\n",
    "    return torch.from_numpy(np.array(frames))\n",
    "\n",
    "# --- H√†m ti·ªÅn x·ª≠ l√Ω cho video m·ªõi ---\n",
    "def preprocess_single_video(video_tensor):\n",
    "    # video_tensor ƒë·∫ßu v√†o c√≥ shape (T, H, W, C) t·ª´ OpenCV\n",
    "    num_frames = video_tensor.shape[0]\n",
    "\n",
    "    if num_frames > MAX_FRAMES:\n",
    "        indices = torch.linspace(0, num_frames - 1, MAX_FRAMES).long()\n",
    "        video_tensor = video_tensor[indices]\n",
    "    elif num_frames < MAX_FRAMES:\n",
    "        padding = torch.zeros((MAX_FRAMES - num_frames,) + video_tensor.shape[1:], dtype=video_tensor.dtype)\n",
    "        video_tensor = torch.cat([video_tensor, padding], dim=0)\n",
    "\n",
    "    # *** THAY ƒê·ªîI QUAN TR·ªåNG ***\n",
    "    # Chuy·ªÉn t·ª´ (T, H, W, C) sang (T, C, H, W) m√† torchvision mong ƒë·ª£i\n",
    "    video_tensor = video_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.Resize(RESIZE_TO),\n",
    "        T.ConvertImageDtype(torch.float32),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Gi·ªù m·ªói frame ƒë√£ c√≥ d·∫°ng (C, H, W)\n",
    "    processed_frames = torch.stack([transform(frame) for frame in video_tensor])\n",
    "    return processed_frames\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ===================================================================\n",
    "    # B∆Ø·ªöC 2: C·∫§U H√åNH V√Ä T·∫¢I M√î H√åNH\n",
    "    # ===================================================================\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MODEL_PATH = \"sign_language_transformer_final.pth\" # S·ª≠ d·ª•ng ƒë√∫ng file tr·ªçng s·ªë\n",
    "    VIDEO_PATH = \"random_10_videos/label_book_idx_3221.mp4\" # <--- THAY ƒê·ªîI VIDEO B·∫†N MU·ªêN KI·ªÇM TRA ·ªû ƒê√ÇY\n",
    "\n",
    "    CLASS_NAMES = [\n",
    "        'again', 'bad', 'bathroom', 'book', 'busy', 'do not want', 'eat', 'father', 'fine', 'finish', \n",
    "        'forget', 'go', 'good', 'happy', 'hello', 'help', 'how', 'i', 'learn', 'like', 'meet', 'milk', \n",
    "        'more', 'mother', 'my', 'name', 'need', 'nice', 'no', 'please', 'question', 'right', 'sad', \n",
    "        'same', 'see you letter', 'thank you', 'want', 'what', 'when', 'where', 'which', 'who', \n",
    "        'why', 'wrong', 'yes', 'you', 'your'\n",
    "    ]\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o ƒê√öNG ki·∫øn tr√∫c m√¥ h√¨nh\n",
    "    model = VideoTransformerClassifier(num_classes=NUM_CLASSES)\n",
    "    \n",
    "    print(\"ƒêang t·∫£i m√¥ h√¨nh...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"T·∫£i m√¥ h√¨nh th√†nh c√¥ng!\")\n",
    "\n",
    "    # ===================================================================\n",
    "    # B∆Ø·ªöC 3: D·ª∞ ƒêO√ÅN V·ªöI VIDEO M·ªöI\n",
    "    # ===================================================================\n",
    "    print(f\"\\nƒêang x·ª≠ l√Ω video: {VIDEO_PATH}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    video_frames_tensor = load_video_frames(VIDEO_PATH)\n",
    "    \n",
    "    if video_frames_tensor.shape[0] == 0:\n",
    "        print(\"K·∫øt th√∫c do kh√¥ng ƒë·ªçc ƒë∆∞·ª£c video.\")\n",
    "    else:\n",
    "        processed_video = preprocess_single_video(video_frames_tensor)\n",
    "        \n",
    "        # Th√™m chi·ªÅu batch (batch_size=1)\n",
    "        # K√≠ch th∆∞·ªõc cu·ªëi c√πng: (1, T, C, H, W)\n",
    "        input_tensor = processed_video.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        print(\"B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n...\")\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            top_prob, top_catid = torch.topk(probabilities, 1)\n",
    "            \n",
    "            predicted_class_index = top_catid[0].item()\n",
    "            predicted_class_name = CLASS_NAMES[predicted_class_index]\n",
    "            prediction_confidence = top_prob[0].item()\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"\\nüöÄ K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\")\n",
    "        print(f\"==> T√™n l·ªõp d·ª± ƒëo√°n: {predicted_class_name}\")\n",
    "        print(f\"==> ƒê·ªô tin c·∫≠y: {prediction_confidence:.2%}\")\n",
    "        print(f\"==> Th·ªùi gian x·ª≠ l√Ω: {end_time - start_time:.2f} gi√¢y\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ª≠ d·ª•ng thi·∫øt b·ªã: cpu\n",
      "ƒêang t·∫£i m√¥ h√¨nh...\n",
      "T·∫£i m√¥ h√¨nh th√†nh c√¥ng!\n",
      "\n",
      "ƒêang x·ª≠ l√Ω video: random_10_videos/label_book_idx_3221.mp4\n",
      "B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n...\n",
      "\n",
      "üöÄ K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN:\n",
      "==> T√™n l·ªõp d·ª± ƒëo√°n: book\n",
      "==> ƒê·ªô tin c·∫≠y: 99.94%\n",
      "==> Th·ªùi gian x·ª≠ l√Ω: 2.23 gi√¢y\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
